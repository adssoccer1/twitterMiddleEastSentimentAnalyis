{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "copy_of_MiddleEast_Twitter_Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adssoccer1/twitterMiddleEastSentimentAnalyis/blob/master/copy_of_MiddleEast_Twitter_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vhm2qtXVOE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tweepy  \n",
        "!pip install tweet-preprocessor\n",
        "!pip install googletrans\n",
        "!python -m nltk.downloader stopwords\n",
        "!python -m nltk.downloader punkt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRu8ihevJCAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#installing necessary packages\n",
        "\n",
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re #regular expression\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "import preprocessor as p\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "#stopwords.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4MZDqpYJFnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Twitter credentials for the app\n",
        "consumer_key = \"\"\n",
        "consumer_secret = \"\"\n",
        "access_key= \"\"\n",
        "access_secret = \"\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ_d2qmjKcWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tweepy as tweepy\n",
        "#pass twitter credentials to tweepy\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzcqsZdvLZUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COLS = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n",
        "'favorite_count', 'retweet_count', 'original_author',   'possibly_sensitive', 'hashtags',\n",
        "'user_mentions', 'place', 'place_coord_boundaries']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F7w3e5ENHxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#yyyy-mm-dd\n",
        "start_date = '2018-05-05'\n",
        "end_date = '2020-02-21'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iBsAZ0nPcHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#file location changed to \"data/telemedicine_data_extraction/\" for clearer path\n",
        "kuwait_tweets = \"/content/sample_data/OMAN2_GREEN.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL9aV_mrLddi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HappyEmoticons\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBnxYqEjLgz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brxaVuPPLqkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine sad and happy emoticons\n",
        "emoticons = emoticons_happy.union(emoticons_sad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtB1v2jLluN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Emoji patterns\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "         u\"\\U000024C2-\\U0001F251\"\n",
        "         \"]+\", flags=re.UNICODE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Auag9XMJvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_tweets(tweet):\n",
        " \n",
        "    stop_words = set(stopwords.words('arabic')) #    stop_words = set(stopwords.words('english')) \n",
        "    word_tokens = word_tokenize(tweet)\n",
        "#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "#replace consecutive non-ASCII characters with a space\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "#remove emojis from tweet\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "#filter using NLTK library append it to a string\n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_tweet = []\n",
        "#looping through conditions\n",
        "    for w in word_tokens:\n",
        "#check tokens against stop words , emoticons and punctuations\n",
        "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "            filtered_tweet.append(w)\n",
        "    return ' '.join(filtered_tweet)\n",
        "    #print(word_tokens)\n",
        "    #print(filtered_sentence)return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tbQG0jHMPQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ar\n",
        "\"\"\"http://docs.tweepy.org/en/v3.5.0/api.html Use this tweepy documentation to specify the\n",
        "search. The search is interfaced with the tweepy.cursor(). can mess around with langs \n",
        "and keywords now....\n",
        "\"\"\"\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "import copy\n",
        "#translator = Translator()\n",
        "import time\n",
        "\n",
        "#set to check for duplicated tweets\n",
        "setOfTweets = set()\n",
        "\n",
        "#method write_tweets()\n",
        "def write_tweets(keyword, file, numTweets):\n",
        "    # If the file exists, then read the existing data from the CSV file.\n",
        "    #if os.path.exists(file):\n",
        "    #     df = pd.read_csv(file, header=0)\n",
        "    #else:\n",
        "    df = pd.DataFrame(columns=COLS)\n",
        "    #page attribute in tweepy.cursor and iteration\n",
        "\n",
        "\n",
        "    csvFile1 = open(file, 'a' ,encoding='utf-8') #to be used at the very end\n",
        "    count = 0\n",
        "    count1 = 0\n",
        "    numSuccessfullyAdded = 0\n",
        "    # REINITIALIZE THE API every page or google blocks us. \n",
        "    translator = Translator()\n",
        "    for page in tweepy.Cursor(api.search, q=keyword, lang='ar', geocode='17.8874,55.6121,300km',\n",
        "                              count=200, include_rts=False, since=start_date).pages(50):\n",
        "\n",
        "        #housekeeping\n",
        "        count = count + 1\n",
        "        print(\"beginning page \", count, \" of 50 tweets each.\")\n",
        "        if(numSuccessfullyAdded >= numTweets):\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "        #this says for tweet in a page of 50 tweets\n",
        "        for status in page:\n",
        "\n",
        "            #terminate if full\n",
        "            if(numSuccessfullyAdded >= numTweets):\n",
        "              print(\"number of desired tweets fullfilled. You should have \", numSuccessfullyAdded, \" tweets in your file.\")\n",
        "              print(\"TERMINATE\")\n",
        "              break\n",
        "\n",
        "            count1 = count1 + 1\n",
        "            print(\"new tweet number: \", count1)\n",
        "            print(\"Successfully added tweets fo far: \", numSuccessfullyAdded)\n",
        "\n",
        "            #sleep for .5 seconds to avoid over requesting from google\n",
        "            time.sleep(1.5)\n",
        "            new_entry = []\n",
        "            status = status._json\n",
        " \n",
        "            ## check whether the tweet is in english or skip to the next tweet\n",
        "            if status['lang'] != 'ar':\n",
        "                continue\n",
        " \n",
        "            #when run the code, below code replaces the retweet amount and\n",
        "            #no of favorires that are changed since last download.\n",
        "            if status['created_at'] in df['created_at'].values:\n",
        "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
        "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
        "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
        "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
        "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
        "                continue\n",
        " \n",
        " \n",
        "            #tweepy preprocessing called for basic preprocessing\n",
        "            clean_text = p.clean(status['text'])\n",
        "\n",
        "            #check the set and skip duplicate tweets even with different ids or authors\n",
        "            if(clean_text in setOfTweets):\n",
        "              print(clean_text)\n",
        "              print(\"   tweet number \", count1, \" has already been seen\")\n",
        "              continue\n",
        "            setOfTweets.add(clean_text)\n",
        " \n",
        "            #call clean_tweet method for extra preprocessing\n",
        "            filtered_tweet=clean_tweets(clean_text)\n",
        "\n",
        "            \n",
        "            #translate the cleaned tweet using google trans\n",
        "            print(\"   Try translating\")            \n",
        "            filtTweetCopy = copy.deepcopy(filtered_tweet)\n",
        "            #print(filtTweetCopy)\n",
        "            try:\n",
        "              result = translator.translate(filtTweetCopy)\n",
        "            except: \n",
        "              print(\"   TRANSLATION FAILED - If lots of these in a row, then google is blocking requests.\")\n",
        "              continue;\n",
        "            print(\"   translating passed\")\n",
        " \n",
        "            #pass textBlob method for sentiment calculations\n",
        "            blob = TextBlob(result.text)\n",
        "            Sentiment = blob.sentiment\n",
        " \n",
        "            #seperate polarity and subjectivity in to two variables\n",
        "            polarity = Sentiment.polarity\n",
        "            subjectivity = Sentiment.subjectivity\n",
        "\n",
        "            print(\"   testing polar\")\n",
        "            if(polarity == 0 or subjectivity == 0):\n",
        "              print(\"   polar failed\")\n",
        "\n",
        "              continue; \n",
        "            print(\"   polar passed\")\n",
        "\n",
        "            #new entry append\n",
        "            new_entry += [status['id'], status['created_at'],\n",
        "                          status['source'], status['text'],result.text, Sentiment,polarity,subjectivity, status['lang'],\n",
        "                          status['favorite_count'], status['retweet_count']]\n",
        " \n",
        "            #to append original author of the tweet\n",
        "            new_entry.append(status['user']['screen_name'])\n",
        " \n",
        "            try:\n",
        "                is_sensitive = status['possibly_sensitive']\n",
        "            except KeyError:\n",
        "                is_sensitive = None\n",
        "            new_entry.append(is_sensitive)\n",
        " \n",
        "            # hashtagas and mentiones are saved using comma separted\n",
        "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
        "            new_entry.append(hashtags)\n",
        "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
        "            new_entry.append(mentions)\n",
        " \n",
        "            #get location of the tweet if possible\n",
        "            try:\n",
        "                location = status['user']['location']\n",
        "            except TypeError:\n",
        "                location = ''\n",
        "            new_entry.append(location)\n",
        " \n",
        "            try:\n",
        "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
        "            except TypeError:\n",
        "                coordinates = None\n",
        "            new_entry.append(coordinates)\n",
        " \n",
        "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
        "            #print(single_tweet_df)\n",
        "            df = df.append(single_tweet_df, ignore_index=True)\n",
        "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
        "            print(\"   SUCCESS with tweet: \", count1)\n",
        "            numSuccessfullyAdded = numSuccessfullyAdded + 1\n",
        "    df.to_csv(csvFile1, mode='a', columns=COLS, index=False, encoding=\"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx1dygO-O7H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OR #Iran OR #Iran OR #Rouhani OR #JCPOA\n",
        "iran_sentiment_keywords = 'إيران OR روحاني OR خامنئي OR جهانگیری OR حاتمي OR ظریف OR واعظی OR #إيران OR #روحاني OR #خامنئي OR #جهانگیری OR #حاتمي OR #ظریف OR #واعظی'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxXhxJtPPSBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The tweets content will only be printed if it is a duplicate so if a \n",
        "#bunch of the same tweets are printed then that means duplicate detection\n",
        "#is working. Everything else that is printed should be self explanatory. \n",
        "#Downloading can take a while now, so the printed \"successfully added tweets so far\"\n",
        "#statement will tell you how far away you are from your max_number_of desired_tweets. \n",
        "\n",
        "\n",
        "\n",
        "#change this number to limit the number of tweets you want to pull. \n",
        "max_number_of_tweets_desired = 1000\n",
        "\n",
        "\n",
        "#call main method passing keywords and file path\n",
        "write_tweets(iran_sentiment_keywords,  kuwait_tweets, max_number_of_tweets_desired)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B81oPiNU-Vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/sample_data/OMAN2_GREEN.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdnfx8Eqsob8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}