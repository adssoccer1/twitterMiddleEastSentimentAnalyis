{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Big Bill's Copy of updatedTwitterData4/21.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adssoccer1/twitterMiddleEastSentimentAnalyis/blob/master/twitterSentimentAnalysisAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyTqudZshDbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enter your Twitter credentials for the app\n",
        "consumer_key = \"\"\n",
        "consumer_secret = \"\"\n",
        "access_key= \"\"\n",
        "access_secret = \"\"\n",
        "\n",
        "#Enter a geolocation and a surrounding km radius as in the example format: '17.8874,55.6121,300km'\n",
        "geoLocation = ''\n",
        "\n",
        "#Please insert both start and end date in the following format: yyyy-mm-dd\n",
        "start_date = ''\n",
        "end_date = ''\n",
        "\n",
        "#insert a save path. For example: \"/content/data/new_data.csv\"\n",
        "save_path_tweets = ''\n",
        "\n",
        "#Insert the language of the tweets you want to search for. You may choose one of the following \n",
        "#supported languages: 'hungarian', 'swedish','kazakh','norwegian', 'finnish', 'arabic','indonesian',\n",
        "#'portuguese', 'turkish','azerbaijani','slovene', 'spanish','danish', 'nepali', 'romanian', 'greek',\n",
        "#'dutch', 'tajik', 'german', 'english', 'russian', 'french', or'italian'\n",
        "language=''\n",
        "\n",
        "\"\"\"write fown key words you want to your sample tweets to contain. \n",
        "Regular words can be formatted: 'hilarious OR funny OR laughing OR #lol' \n",
        "Keywords preceeded by the # will looks for hashtags with that keyword.\n",
        "Here is an example of arabic key words: 'جهانگیری OR حاتمي OR ظریف OR واعظی OR #إيران OR #روحاني OR #خامنئي OR #جهانگیری OR #حاتمي '\n",
        "Notice the keywords are surrounded by ''. \n",
        "Make sure your keywords match the designated language.\n",
        "\"\"\"\n",
        "sentiment_keywords = ''\n",
        "\n",
        "#insert the number of tweets you want to sample from\n",
        "max_number_of_tweets_desired = 100\n",
        "\n",
        "#Now you can run the remaining cells. \n",
        "#A tweet's content will only be printed if it is a duplicate. So if a \n",
        "#bunch of the same tweets are printed then that means duplicate detection\n",
        "#is working. Everything else that is printed should be self explanatory. \n",
        "#Downloading can take a while now, so the printed \"successfully added tweets so far\"\n",
        "#statement will tell you how far away you are from your max_number_of desired_tweets. \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vhm2qtXVOE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tweepy  \n",
        "!pip install tweet-preprocessor\n",
        "!pip install googletrans\n",
        "!python -m nltk.downloader stopwords\n",
        "!python -m nltk.downloader punkt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRu8ihevJCAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#installing necessary packages, ect.\n",
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re #regular expression\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import os\n",
        "import preprocessor as p\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "#stopwords.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ_d2qmjKcWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tweepy as tweepy\n",
        "#pass twitter credentials to tweepy\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzcqsZdvLZUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The columns of data the search will put into the csv file\n",
        "COLS = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n",
        "'favorite_count', 'retweet_count', 'original_author',   'possibly_sensitive', 'hashtags',\n",
        "'user_mentions', 'place', 'place_coord_boundaries']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL9aV_mrLddi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HappyEmoticons\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBnxYqEjLgz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brxaVuPPLqkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine sad and happy emoticons\n",
        "emoticons = emoticons_happy.union(emoticons_sad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtB1v2jLluN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Emoji patterns\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "         u\"\\U000024C2-\\U0001F251\"\n",
        "         \"]+\", flags=re.UNICODE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Auag9XMJvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_tweets(tweet, language):\n",
        "    stop_words = set(stopwords.words(language)) \n",
        "    word_tokens = word_tokenize(tweet)\n",
        "#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "#replace consecutive non-ASCII characters with a space\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "#remove emojis from tweet\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "#filter using NLTK library append it to a string\n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_tweet = []\n",
        "#looping through conditions\n",
        "    for w in word_tokens:\n",
        "#check tokens against stop words , emoticons and punctuations\n",
        "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "            filtered_tweet.append(w)\n",
        "    return ' '.join(filtered_tweet)\n",
        "    #print(word_tokens)\n",
        "    #print(filtered_sentence)return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tbQG0jHMPQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"http://docs.tweepy.org/en/v3.5.0/api.html Use this tweepy documentation to specify the\n",
        "search. The search is interfaced with the tweepy.cursor(). can mess around with langs \n",
        "and keywords now....\n",
        "\"\"\"\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "import copy\n",
        "import time\n",
        "\n",
        "#helper function to validate supported langauges\n",
        "def validLanguage(language):\n",
        "  setOfLangCodes = {'hungarian':'hu',\n",
        "  'swedish':'sv' ,\n",
        "  'kazakh':'kk',\n",
        "  'norwegian':'no',\n",
        "  'finnish':'fi',\n",
        "  'arabic':'ar',\n",
        "  'indonesian':'id',\n",
        "  'portuguese':'pt',\n",
        "  'turkish':'tr',\n",
        "  'azerbaijani':'az',\n",
        "  'slovene':'sl',\n",
        "  'spanish':'es',\n",
        "  'danish':'da',\n",
        "  'nepali': 'ne',\n",
        "  'romanian':'ro',\n",
        "  'greek':'el',\n",
        "  'dutch':'nl',\n",
        "  'tajik':'tg',\n",
        "  'german':'de',\n",
        "  'english' :'en',\n",
        "  'russian':'ru',\n",
        "  'french' :'fr',\n",
        "  'italian':'it'}\n",
        "  if language not in setOfLangCodes:\n",
        "    return ''\n",
        "  else:\n",
        "    return setOfLangCodes.get(language)\n",
        "\n",
        "#main method\n",
        "def write_tweets(keyword, file, numTweets, geoLocation, language, langCode, setOfTweets):\n",
        "\n",
        "    df = pd.DataFrame(columns=COLS)\n",
        "\n",
        "    csvFile1 = open(file, 'a' ,encoding='utf-8') #to be used at the very end\n",
        "    count = 0\n",
        "    count1 = 0\n",
        "    numSuccessfullyAdded = 0\n",
        "\n",
        "    # REINITIALIZE THE TRANSLATION API every page or google blocks requests. \n",
        "    translator = Translator()\n",
        "\n",
        "    for page in tweepy.Cursor(api.search, q=keyword, lang=langCode, geocode=geoLocation,\n",
        "                              count=200, include_rts=False, since=start_date).pages(50):\n",
        "\n",
        "        #housekeeping\n",
        "        count = count + 1\n",
        "        print(\"beginning page \", count, \" of 50 tweets each.\")\n",
        "        if(numSuccessfullyAdded >= numTweets):\n",
        "          break\n",
        "\n",
        "        #this says for tweet in a page of 50 tweets\n",
        "        for status in page:\n",
        "\n",
        "            #terminate if full\n",
        "            if(numSuccessfullyAdded >= numTweets):\n",
        "              print(\"number of desired tweets fullfilled. You should have \", numSuccessfullyAdded, \" tweets in your file.\")\n",
        "              print(\"TERMINATE\")\n",
        "              break\n",
        "\n",
        "            count1 = count1 + 1\n",
        "            print(\"new tweet number: \", count1)\n",
        "            print(\"Successfully added tweets fo far: \", numSuccessfullyAdded)\n",
        "\n",
        "            #sleep for .5 seconds to avoid over requesting from google\n",
        "            time.sleep(1.5)\n",
        "\n",
        "            new_entry = []\n",
        "            status = status._json\n",
        " \n",
        "            #check whether the tweet is in desired language or skip to the next tweet\n",
        "            if status['lang'] != langCode:\n",
        "                continue\n",
        " \n",
        "            #when run the code, below code replaces the retweet amount and\n",
        "            #no of favorires that are changed since last download.\n",
        "            if status['created_at'] in df['created_at'].values:\n",
        "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
        "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
        "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
        "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
        "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
        "                continue\n",
        " \n",
        " \n",
        "            #tweepy preprocessing called for basic preprocessing\n",
        "            clean_text = p.clean(status['text'])\n",
        "\n",
        "            #check the set and skip duplicate tweets even with different ids or authors\n",
        "            if(clean_text in setOfTweets):\n",
        "              print(clean_text)\n",
        "              print(\"   tweet number \", count1, \" has already been seen\")\n",
        "              continue\n",
        "            setOfTweets.add(clean_text)\n",
        " \n",
        "            #call clean_tweet method for extra preprocessing\n",
        "            filtered_tweet=clean_tweets(clean_text, language)\n",
        "\n",
        "            #translate the cleaned tweet using google trans\n",
        "            print(\"   Try translating\")            \n",
        "            filtTweetCopy = copy.deepcopy(filtered_tweet)\n",
        "            try:\n",
        "              result = translator.translate(filtTweetCopy)\n",
        "            except: \n",
        "              print(\"   TRANSLATION FAILED - If lots of these in a row, then google trans is blocking requests.\")\n",
        "              continue;\n",
        "            print(\"   translating passed\")\n",
        " \n",
        "            #pass textBlob method for sentiment analysis\n",
        "            blob = TextBlob(result.text)\n",
        "            Sentiment = blob.sentiment\n",
        " \n",
        "            #seperate polarity and subjectivity in to two variables\n",
        "            polarity = Sentiment.polarity\n",
        "            subjectivity = Sentiment.subjectivity\n",
        "\n",
        "            #Dont include any tweets with 0 subjectivity or polarity ratings. \n",
        "            print(\"   testing polar\")\n",
        "            if(polarity == 0 or subjectivity == 0):\n",
        "              print(\"   polar failed\")\n",
        "              continue; \n",
        "            print(\"   polar passed\")\n",
        "\n",
        "            #new entry append\n",
        "            new_entry += [status['id'], status['created_at'],\n",
        "                          status['source'], status['text'], result.text, Sentiment, polarity, subjectivity, status['lang'],\n",
        "                          status['favorite_count'], status['retweet_count']]\n",
        " \n",
        "            #to append original author of the tweet\n",
        "            new_entry.append(status['user']['screen_name'])\n",
        " \n",
        "            try:\n",
        "                is_sensitive = status['possibly_sensitive']\n",
        "            except KeyError:\n",
        "                is_sensitive = None\n",
        "            new_entry.append(is_sensitive)\n",
        " \n",
        "            # hashtagas and mentiones are saved using comma separted\n",
        "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
        "            new_entry.append(hashtags)\n",
        "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
        "            new_entry.append(mentions)\n",
        " \n",
        "            #get location of the tweet if possible\n",
        "            try:\n",
        "                location = status['user']['location']\n",
        "            except TypeError:\n",
        "                location = ''\n",
        "            new_entry.append(location)\n",
        " \n",
        "            try:\n",
        "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
        "            except TypeError:\n",
        "                coordinates = None\n",
        "            new_entry.append(coordinates)\n",
        " \n",
        "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
        "\n",
        "            df = df.append(single_tweet_df, ignore_index=True)\n",
        "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
        "\n",
        "            print(\"   SUCCESS with tweet: \", count1)\n",
        "            numSuccessfullyAdded = numSuccessfullyAdded + 1\n",
        "    df.to_csv(csvFile1, mode='a', columns=COLS, index=False, encoding=\"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxXhxJtPPSBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The tweets content will only be printed if it is a duplicate so if a \n",
        "#bunch of the same tweets are printed then that means duplicate detection\n",
        "#is working. Everything else that is printed should be self explanatory. \n",
        "#Downloading can take a while now, so the printed \"successfully added tweets so far\"\n",
        "#statement will tell you how far away you are from your max_number_of desired_tweets. \n",
        "\n",
        "\n",
        "#process the langague to see if its supported and get the lang code. Then call the write tweets method.\n",
        "langCode = validLanguage(language)\n",
        "print(langCode)\n",
        "if langCode == '':\n",
        "  print(\"Invalid language specification\")\n",
        "else:\n",
        "  #call main method passing keywords and file path\n",
        "  setOfSeenTweets = set()\n",
        "  write_tweets(sentiment_keywords,  save_path_tweets, max_number_of_tweets_desired, geoLocation, language, langCode, setOfSeenTweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B81oPiNU-Vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(save_path_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}